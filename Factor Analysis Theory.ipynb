{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa576f9",
   "metadata": {},
   "source": [
    "# Factor analysis\n",
    "\n",
    "## Why factor analysis?\n",
    "\n",
    "When we have data $x^{(i)} \\in \\mathbb{R} ^n$ coming from a mixture of several Gaussians, we can use the EM Algorithm to fit the mixture model. However, we usually have sufficient data to discern the multiple-Gaussian structure.\n",
    "\n",
    "Assuming we have $m$ observations of $n$ dimensional data. Consider a setting in which $n >> m$. If we model the data as Gaussian and extimate the mean and covarience as usual maximum likelihood estimators,\n",
    "\n",
    "$$\\mu = \\frac{1}{m}\\sum_{i=1}^mx^{(i)}$$\n",
    "$$\\Sigma = \\frac{1}{m}\\sum_{i=1}^m(x^{(i)}-\\mu)(x^{(i)}-\\mu)^T$$\n",
    "we would find that matrix $\\Sigma$ is singular. Therefore, the Gaussian probability distribution function cannot be estimated.\n",
    "\n",
    "More generally, unless $m$ exceeds $n$ by reasonable amount, the maximum likelihood estimates of the mean and covariance may be quite poor. In this case, we can use the Factor Analysis model to analyse underlying factor and reduce dimensionality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95307bbf",
   "metadata": {},
   "source": [
    "## The Factor analysis model\n",
    "\n",
    "In the Factor analysis model, we create a joint probability distribution on $(x, z)$ where $z \\in \\mathbb{R}^k$ is a latent random variable, as follows:\n",
    "\n",
    "$$ z \\sim \\mathcal{N}(0, I)$$\n",
    "$$ x|z \\sim \\mathcal{N}(\\mu+\\Lambda z, \\Psi)$$\n",
    "\n",
    "the parameters of our model being $\\mu \\in \\mathbb{R}^n$, the matrix, $\\Lambda\\in\\mathbb{R}^{n\\times k}$ and the diagonal matrix $\\Psi\\in\\mathbb{R}^{n\\times n}$.\n",
    "\n",
    "We can also define the factor analysis model according to\n",
    "\n",
    "$$z\\sim\\mathcal{N}(0,I)$$\n",
    "$$\\epsilon\\sim\\mathcal{N}(0, \\Psi)$$\n",
    "$$x = \\mu + \\Lambda z + \\epsilon$$\n",
    "\n",
    "The random variables $z$ and $x$ have joint Gaussian distribution\n",
    "$$\\begin{bmatrix}\n",
    "z\\\\\n",
    "x\n",
    "\\end{bmatrix}\\sim \\mathcal{N}(\\mu_{zx}, \\Sigma)$$\n",
    "\n",
    "$E[z]=0$ because $z\\sim\\mathcal{N}(0,I)$\n",
    "\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "E[x]&= E[\\mu+\\Lambda z+ \\epsilon] \\\\\n",
    "    &= \\mu + \\Lambda E[z]+E[\\epsilon] \\\\\n",
    "    &= \\mu\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore,\n",
    "$$\\mu_{zx} = \\begin{bmatrix}\n",
    "\\overrightarrow{0}\\\\\n",
    "\\mu\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Now, to find $\\Sigma$ where \n",
    "$$ \\Sigma = \\begin{bmatrix}\n",
    "\\Sigma_{zz} & \\Sigma_{zx} \\\\\n",
    "\\Sigma_{xz} & \\Sigma_{xx} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We calculate $\\Sigma_{zz}=E[(z-E[z])(z-E[z])^T]$, $\\Sigma_{zx}=E[(z-E[z])(x-E[x])^T]$, $\\Sigma_{xz}=E[(x-E[x])(z-E[z])^T]$ and $\\Sigma_{xx}=E[(x-E[x])(x-E[x])^T]$\n",
    "\n",
    "After calcualting all these values, we obtain:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "z\\\\\n",
    "x\n",
    "\\end{bmatrix}\\sim\n",
    "\\mathcal{N}\\left(\\begin{bmatrix}\n",
    "\\overrightarrow{0}\\\\\n",
    "\\mu\n",
    "\\end{bmatrix}, \\begin{bmatrix}\n",
    "I & \\Lambda^T \\\\\n",
    "\\Lambda & \\Lambda\\Lambda^T+\\Psi\n",
    "\\end{bmatrix}\\right)$$\n",
    "\n",
    "We can see that the marginal distribution of $x$ is given by $x\\sim\\mathcal{N}(\\mu,\\Lambda\\Lambda^T+\\Psi)$\n",
    "\n",
    "If given a training set $\\{x^{(i)};i=1,...,m\\}$, the log likelihood of the parameters will be\n",
    "\n",
    "$$l(\\mu,\\Lambda,\\Psi) = log\\prod_{i=1}^m\\frac{1}{(2\\pi)^{n/2}|\\Lambda\\Lambda^T+\\Psi|}exp\\left(-\\frac{1}{2}(x^{(i)}-\\mu)^T(\\Lambda\\Lambda^T+\\Psi)^{-1}(x^{(i)}-\\mu)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472b546",
   "metadata": {},
   "source": [
    "## EM For Factor Analysis\n",
    "\n",
    "### E - Step:\n",
    "We need to compute $$Q_{i}(z^{(i)}) = p(z^{(i)}|x^{(i)}; \\mu,\\Lambda,\\psi)$$\n",
    "\n",
    "\n",
    "Substituting distribution in Equation (3) into formulas (1-2), we find that,\n",
    "\n",
    "$$z^{(i)}|x^{(i)}; \\mu,\\Lambda,\\psi \\backsim N(\\mu_{z^{(i)}|x^{(i)}}, \\Sigma_{z^{(i)}|x^{(i)}})$$\n",
    "\n",
    "\n",
    "where,\n",
    "\n",
    "$$ \\mu_{z^{(i)}|x^{(i)}} = \\Lambda^{T}(\\Lambda\\Lambda^{T}+\\psi)^{-1}(x^{(i)}-\\mu) $$\n",
    "$$ \\Sigma_{z^{(i)}|x^{(i)}} = I - \\Lambda^{T}(\\Lambda\\Lambda^{T}+\\psi)^{-1}\\Lambda $$\n",
    "\n",
    "Using the above definitions, we have frac{k}{2}}\n",
    "$$ Q_{i}(z^{(i)}) = \\frac{1}{(2\\pi)^{k/2} |\\Sigma_{z^{(i)}|x^{(i)}}|^{1/2}}exp\\bigg(-\\frac{1}{2}(z^{(i)} - \\mu_{z^{(i)}|x^{(i)}})^{T}\\Sigma^{-1}_{z^{(i)}|x^{(i)}}(z^{(i)} - \\mu_{z^{(i)}|x^{(i)}})\\bigg)$$\n",
    "\n",
    "\n",
    "### M - Step:\n",
    "For M-step, we need to maximize:\n",
    "$$ \\sum_{i=1}^{m} \\int_{z^{(i)}} Q_{i}(z^{(i)}) log \\frac{p(x^{(i)}, z^{(i)}; \\mu, \\Lambda, \\psi)}{Q_{i}(z^{(i)})}dz^{(i)}$$\n",
    "with respect to $\\mu, \\Lambda, \\psi$\n",
    "\n",
    "On solving for $\\Lambda$ we will get the following expression:\n",
    "\n",
    "$$\\Lambda = \\bigg(\\sum_{i=1}^{m}(x^{(i)} - \\mu) E_{z^{(i)}\\backsim Q_{i}}\\bigg[z^{(i)}(z^{(i)})^{T}\\bigg]\\bigg)^{-1}$$\n",
    "\n",
    "Note the close relationship between this equation and Normal Equation:\n",
    "$$\\theta^{T} = (y^{T}X)(X^{T}X)^{-1}$$\n",
    "The analogy here is that x's are the linear function of z's (plus noise).\n",
    "<br>Given the guesses for z from E-step we will now try to estimate the unknown linearity $\\Lambda$ relating the x's and z's  \n",
    "\n",
    "<br>\n",
    "The M-step update for Lambda\n",
    "\n",
    "$$\\Lambda = \\bigg(\\sum_{i=1}^{m}(x^{(i)}-\\mu)\\mu^{T}_{z^{(i)}|x^{(i)}}\\bigg)\\bigg(\\sum_{i=1}^{m}\\mu_{z^{(i)}|x^{(i)}}\\mu^{T}_{z^{(i)}|x^{(i)}}+\\Sigma_{z^{(i)}|x^{(i)}}\\bigg)^{-1}$$\n",
    "\n",
    "Lastly, M-step optimization steps for the other parameters can be given by:\n",
    "$$\\mu = \\frac{1}{m}\\sum_{i=1}^{m}x^{(i)}$$ This can be calculated just once and needs not to be further updated.\n",
    "\n",
    "Similarly,\n",
    "$$\\psi = \\frac{1}{m}\\sum_{i=1}^{m}x^{(i)}(x^{(i)})^{T}-x^{(i)}\\mu^{T}_{z^{(i)}|x^{(i)}}\\Lambda^{T}-\\Lambda\\mu_{z^{(i)}|x^{(i)}}(x^{(i)})^{T} + \\Lambda(\\mu_{z^{(i)}|x^{(i)}}\\mu^{T}_{z^{(i)}|x^{(i)}} + \\Sigma_{z^{(i)}|x^{(i)}})\\Lambda^{T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
